<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>PySpark and Parquet: Elegant Python DataFrames and SQL &mdash; CodeSolid.com 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=19645805"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex/" />
    <link rel="search" title="Search" href="../search/" />
    <link rel="next" title="Python Dataclass: Easily Automate Class Best Practices" href="../python-dataclasses/" />
    <link rel="prev" title="Python Dunder Methods: The Ugliest Awesome Sauce" href="../dunder-methods-in-python-the-ugliest-awesome-sauce/" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../" class="icon icon-home">
            CodeSolid.com
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Featured Articles:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../using-latex-in-python/">Using LaTeX In Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installing-pyenv-on-a-mac/">Installing Pyenv on a Mac (A Setup Guide With Usage Tips)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../conda-vs-pip/">Conda vs. Pip, Venv, and Pyenv – Simplicity Wins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jupyter-password/">Jupyter Password: Easy Notebook and Lab Configuration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Categories</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../category-math-and-math-software/">Math and Math Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../category-math/">Math</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../category-python/">Python (General)</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../why-is-python-popular/">Why Is Python Popular?# Why Is Python Popular</a></li>
<li class="toctree-l2"><a class="reference internal" href="../benchmarking-python-and-rust-async-web-server-performance/">Benchmarking Python and Rust Async Web Server Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-date-tutorial/">Python Date and Time Functions: The Complete Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to-convert-a-string-to-datetime-in-python/">How to Convert a String to Datetime in Python?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to-use-docker-with-python/">How To Use Docker Python Images and Docker Compose With Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../debugging-python-in-vs-code/">Debugging Python in VS Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../zen-of-100-days-of-python/">The Strange Zen of 100 Days of Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../airflow-python-etl/">Apache Airflow:  Python ETL and Scheduling Made Easy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../finding-duplicates-in-a-list-in-python/">How to Find Duplicates In a List in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-generators-questions-and-answers/">Python Generator Functions: The Complete Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-functions/">Python Functions: Introduction to the Series</a></li>
<li class="toctree-l2"><a class="reference internal" href="../using-latex-in-python/">Using LaTeX In Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../introducing-sage-math-python-based-mathematics/">Introducing Sage Math:  Symbolic Math Software In Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-virtual-environments-video-tutorial/">Python Virtual Environments: Video Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../understanding-python-types-and-type-hints/">Understanding Python Types and Type Hints</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-errors/">Python Errors: Exception Handling from Beginner to Expert</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-do-i-profile-python-code/">How To Profile Python Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-dictionaries-with-exercises/">Python Dictionaries for Beginners:  A Complete Lesson With Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-for-loop-a-complete-tutorial-with-exercises/">The Python For Loop:  Complete Tutorial and Practice Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../exploring-python-objects-with-the-dir-and-type-functions/">Exploring Python Objects with the dir and type functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../google-sheets-in-python-and-pandas/">How to Work With Google Sheets In Python and Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../what-is-a-python-package/">What Is a Python Package?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learn-python/">Learn Python: Tutorials from Beginner to Expert</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-indexing-slicing-exercises/">Python Indexing and Slicing: Complete Tutorial With Hands-On Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../julia-vs-python-now-for-something-completely-different/">Is Julia Easy to Learn for Python Developers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-format-strings/">Python Format Strings: Beginner to Expert</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-files-from-beginner-to-expert/">Find Files in Python:  Complete Cookbook for Searching Files and Folders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-shell-programming/">Python Shell Programming: Overview and Top Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../is-python-slow/">Is Python Slow?  Separating the Myths from the Facts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../what-are-those-pyc-files-in-a-python-project/">What Are pyc Files and <strong>pycache</strong> folders In Python ?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../writing-a-python-custom-exception/">Writing a Python Custom Exception</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-lists-vs-arrays/">Python Lists vs. Arrays:  How to Choose Between Them</a></li>
<li class="toctree-l2"><a class="reference internal" href="../useful-collection-classes-in-python-you-may-not-know/">Useful Collection Classes in Python You May Not Know</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pypy-first-look-a-faster-version-of-python/">Is PyPy a Faster Version of Python?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-and-aws-lambda-functions/">Python for AWS Lambda Functions: A Beginner’s Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dunder-methods-in-python-the-ugliest-awesome-sauce/">Python Dunder Methods:  The Ugliest Awesome Sauce</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">PySpark and Parquet: Elegant Python DataFrames and SQL</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction">Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#what-we-ll-cover">What We’ll Cover</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#what-is-pyspark">What Is PySpark?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#installing-pyspark-and-jupyterlab">Installing PySpark and JupyterLab</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#installing-with-pip-and-venv">Installing With Pip and Venv</a></li>
<li class="toctree-l4"><a class="reference internal" href="#installing-with-conda">Installing with Conda</a></li>
<li class="toctree-l4"><a class="reference internal" href="#testing-the-installation">Testing The Installation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#reading-parquet-files-in-pyspark">Reading Parquet Files in PySpark</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#using-spark-s-read-method">Using Spark’s Read Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="#querying-a-file-with-spark-sql">Querying a File with Spark SQL</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#how-to-improve-pyspark-s-dataframes-display">How To Improve PySpark’s DataFrames Display</a></li>
<li class="toctree-l3"><a class="reference internal" href="#writing-parquet-files">Writing Parquet Files</a></li>
<li class="toctree-l3"><a class="reference internal" href="#partitioning-parquet-files">Partitioning Parquet Files</a></li>
<li class="toctree-l3"><a class="reference internal" href="#parquet-vs-pyspark-tables">Parquet vs. PySpark Tables</a></li>
<li class="toctree-l3"><a class="reference internal" href="#closing-thoughts">Closing Thoughts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../python-dataclasses/">Python Dataclass:  Easily Automate Class Best Practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../sqlalchemy-dataclass-example/">SQLAlchemy DataClass Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-with-using-and-writing-context-managers-in-python/">Python With: Using and Writing Context Managers in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../random-python-secrets-and-random-values-made-easy/">Random Python:  Secrets and Random Values Made Easy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-docker-examples-sagemath-in-a-container/">Python Docker Examples:  SageMath in a Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="../scipy-vs-sympy-for-symbolic-math-let-us-never-speak-of-this-again/">SciPy vs. SymPy for Symbolic Math:  Let Us Never Speak of This Again</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to-compare-python-dictionaries/">How To Compare Python Dictionaries</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to-separate-tests-and-source-for-python-tests/">How To Separate Source and Tests in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning-c-and-python-the-perfect-duo-for-success/">Learning C++ and Python: The Perfect Duo for Success</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-operators/">Python Operators:  The Building Blocks of Successful Code</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../category-python-for-beginners-posts/">Python for Beginners</a></li>
<li class="toctree-l1"><a class="reference internal" href="../category-python-functions/">Python Funcitons</a></li>
<li class="toctree-l1"><a class="reference internal" href="../category-python-math-and-science/">Python Math and Science</a></li>
<li class="toctree-l1"><a class="reference internal" href="../category-python-practice/">Python Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../category-python-tools/">Python Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../category-pandas/">Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="../category-newsletter/">Newsletter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../category-miscellaneous/">Other</a></li>
<li class="toctree-l1"><a class="reference internal" href="../category-c-and-cplusplus/">C and C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../category-docker/">Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../category-jupyter/">Jupyter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../category-learn-to-code/">Learn to Code</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../">CodeSolid.com</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../category-python/">Python (General)</a></li>
      <li class="breadcrumb-item active">PySpark and Parquet: Elegant Python DataFrames and SQL</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/using-pyspark-and-parquet.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="pyspark-and-parquet-elegant-python-dataframes-and-sql">
<h1>PySpark and Parquet: Elegant Python DataFrames and SQL<a class="headerlink" href="#pyspark-and-parquet-elegant-python-dataframes-and-sql" title="Link to this heading"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>In a recent article on <a class="reference external" href="https://codesolid.com/python-pyarrow-and-parquet/">Python PyArrow and Parquet</a>, we discussed the speed and size advantages of the Parquet data format and the performance of PyArrow compared to Pandas. We also compared the performance of Pandas, Polars, and native PyArrow for working with Parquet files.</p>
<p>Since then, two things have captured our attention (one old, one new):</p>
<ul class="simple">
<li><p>Another important Python project where Parquet is heavily used and well supported is PySpark, which we wanted to revisit and have not yet discussed much on this blog. Parquet also has a dependency on PyArrow.</p></li>
<li><p>Pandas has begun shipping release candidates for Python 2.0.0, which now features an option for using PyArrow rather than NumPy internally.</p></li>
</ul>
<p>Since we already wanted to see how PySpark handled large data sets in Python as a follow-up to our <a class="reference external" href="https://codesolid.com/large-data-sets-in-python-pandas-and-the-alternatives/">earlier article</a> of the same name, we thought this would be an excellent time to see how PySpark’s API and performance compared to that of not only PyArrow and Polars, which we’ve already investigated but also the new and improved Pandas.</p>
<section id="what-we-ll-cover">
<h3>What We’ll Cover<a class="headerlink" href="#what-we-ll-cover" title="Link to this heading"></a></h3>
<p>To help get you started with PySpark, we’ll first go over installing it and configuring a must-have SparkSession object in JupyterLab. After this, some of the topics we’ll cover include:</p>
<ul class="simple">
<li><p>What Is PySpark?</p></li>
<li><p>Installing PySpark via pip or conda</p></li>
<li><p>Basic loading and saving of Parquet files</p></li>
<li><p>Improving the display</p></li>
<li><p>Partitioning</p></li>
<li><p>Querying directly from Parquet files</p></li>
<li><p>How PySpark stores table data</p></li>
</ul>
</section>
</section>
<section id="what-is-pyspark">
<h2>What Is PySpark?<a class="headerlink" href="#what-is-pyspark" title="Link to this heading"></a></h2>
<p>Python is a Python interface to Spark, a tool for doing data analytics, data engineering, and machine learning on local machines or clusters of machines. For this reason, unlike Pandas, which does not scale well to clusters without third-party libraries, PySpark supports data analysis at scale “out of the box,” so to speak.</p>
<p>Spark itself is written in Scala, so the fact that PySpark is an interface to the Spark libraries means we’ll need one additional dependency before we install the Python library.</p>
</section>
<section id="installing-pyspark-and-jupyterlab">
<h2>Installing PySpark and JupyterLab<a class="headerlink" href="#installing-pyspark-and-jupyterlab" title="Link to this heading"></a></h2>
<p>The Scala code that Spark is written in targets the Java Virtual Machine (JVM), so before installing PySpark, we’ll need to ensure we have at least Java 8 installed on our system. I have the JDK (the Java Development Kit), but if space is a concern, you can probably get by with just the JRE (Java Runtime Environment), which is smaller. You can download either option from the <a class="reference external" href="https://www.oracle.com/java/technologies/downloads/">Java downloads page.</a></p>
<p>Some PySpark documentation claims you’ll also need to set JAVA_HOME. Still, I’ve verified both by testing and by reviewing the PySpark code that if that isn’t set, having the java executable on your path is sufficient. You can verify this using <code class="docutils literal notranslate"><span class="pre">java</span> <span class="pre">--version</span></code>.</p>
<p>With Java available, we can install a basic PySpark plus some basic tools to give us a JupyterLab environment using either Pip or Conda.</p>
<section id="installing-with-pip-and-venv">
<h3>Installing With Pip and Venv<a class="headerlink" href="#installing-with-pip-and-venv" title="Link to this heading"></a></h3>
<p>Here’s how to install what you need using pip and the venv module:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>venv<span class="w"> </span>

<span class="c1"># On Mac/Linux</span>
<span class="nb">source</span><span class="w"> </span>venv/bin/activate

<span class="c1"># On Windows</span>
.<span class="se">\v</span>env<span class="se">\S</span>cripts<span class="se">\a</span>ctivate

<span class="c1"># On any system</span>
pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>pip
pip<span class="w"> </span>install<span class="w"> </span>pyspark<span class="w"> </span>ipython<span class="w"> </span>jupyterlab
</pre></div>
</div>
</section>
<section id="installing-with-conda">
<h3>Installing with Conda<a class="headerlink" href="#installing-with-conda" title="Link to this heading"></a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>pyspark<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.11<span class="w"> </span>pyspark<span class="w"> </span>ipython<span class="w"> </span>jupyterlab
conda<span class="w"> </span>activate<span class="w"> </span>pyspark
</pre></div>
</div>
</section>
<section id="testing-the-installation">
<h3>Testing The Installation<a class="headerlink" href="#testing-the-installation" title="Link to this heading"></a></h3>
<p>Perhaps the easiest way to run PySpark is to use the PySpark shell, which will create a SparkSession behind the scenes for you. Simply type Spark from one of the environment</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>% pyspark
Python 3.11.0 | packaged by conda-forge | (main, Jan 14 2023, 12:25:12) [Clang 14.0.6 ] on darwin
...
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  &#39;_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.3.2
      /_/

...
SparkSession available as &#39;spark&#39;.
</pre></div>
</div>
<p>That’s all looking good. In JupyterLab or a Python script, we’ll need to set up the SparkSession object ourselves, but it’s easy to do. Use <code class="docutils literal notranslate"><span class="pre">jupyter</span> <span class="pre">lab</span></code> to run Jupyter, then create a new Python 3 notebook. In a cell, the following code will create a SparkContext:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="n">spark</span> <span class="o">=</span> <span class="p">(</span><span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;pyspark_parquet&quot;</span><span class="p">)</span>
     <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.sql.crossJoin.enabled&quot;</span><span class="p">,</span><span class="s2">&quot;true&quot;</span><span class="p">)</span>
     <span class="o">.</span><span class="n">getOrCreate</span><span class="p">())</span>
</pre></div>
</div>
<p>You’ll see some warnings that can be safely ignored.</p>
<p><img alt="" src="../_images/image-12-1024x160.png" /></p>
<p>By the way, if you’re curious about the extra parentheses before SparkSession and on the last line, this is just a trick in Python, so we can write this “one-liner” as a multi-line expression. Since PySpark makes extensive use of the builder pattern so we can chain expressions, you’ll find this frequently in PySpark code.</p>
<p>As a final test, you can run a query to ensure everything works correctly. Using SQL in Spark is easy. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;select current_date() as today;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Output (example):</p>
<p><img alt="Output showing current_date() in PySpark" src="../_images/image-133.png" /></p>
</section>
</section>
<section id="reading-parquet-files-in-pyspark">
<h2>Reading Parquet Files in PySpark<a class="headerlink" href="#reading-parquet-files-in-pyspark" title="Link to this heading"></a></h2>
<p>OK, enough sanity checks – we’re ready to start working with Parquet files. Parquet is a fast, binary, column-based storage format that supports compression. Throughout this article, we’ll use the taxis dataset (<a class="reference external" href="https://github.com/CodeSolid/pyspark-demo/raw/main/taxis.parquet">download file</a>), which is based on the CSV Seaborn sample example.</p>
<p>In PySpark, there are at least two ways to work with an existing Parquet file.</p>
<section id="using-spark-s-read-method">
<h3>Using Spark’s Read Method<a class="headerlink" href="#using-spark-s-read-method" title="Link to this heading"></a></h3>
<p>First, we can load the file into a PySpark DataFrame in memory using the <code class="docutils literal notranslate"><span class="pre">spark.read</span></code> method. Here we do that and print out some basic information about the DataFrame.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">taxis</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;parquet&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;./taxis.parquet&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">taxis</span><span class="p">))</span>
<span class="n">taxis</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rows: </span><span class="si">{</span><span class="n">taxis</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<p><img alt="Displaying basic information about a PySpark DataFrame" src="../_images/image-143.png" /></p>
</section>
<section id="querying-a-file-with-spark-sql">
<h3>Querying a File with Spark SQL<a class="headerlink" href="#querying-a-file-with-spark-sql" title="Link to this heading"></a></h3>
<p>Loading a file into a DataFrame like this is a commonplace operation in Pandas, PolaRS, and many other tools, but where Spark really shines is in SparkSQL’s ability to query files like Parquet files directly. For example, let’s say we want to know how far a cabbie has to drive to earn their top fares. Here’s how we might get at that answer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">top_fares</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT fare, tip, fare + tip as total, distance, tolls from parquet.`taxis.parquet` order by fare desc limit 10&quot;</span><span class="p">)</span>
<span class="n">top_fares</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Output:</p>
<p><img alt="Using the show method on PySpark's DataFrame object." src="../_images/image-153.png" /></p>
<p>Notice that we’ve still created a DataFrame here as “top_fares,” but now we’ve used far less memory for it than when we loaded the entire file into memory. Of course, we’ve already loaded as df, so we might want to run the same query against it instead. We can’t do that directly, but we can create a temporary “view” of it and query that instead.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">taxis</span><span class="o">.</span><span class="n">createOrReplaceGlobalTempView</span><span class="p">(</span><span class="s2">&quot;taxis&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT fare, tip, fare + tip as total, distance, tolls from global_temp.taxis order by fare desc limit 10&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>The output would be the same as shown above.</p>
<p>And speaking of the output, the ASCII-ish view shown above would be fine for something like the PySpark shell, but in a notebook, it isn’t as pretty as what we get for free in Pandas or PolaRS. Let’s fix that now by taking a short detour from our main topic.</p>
</section>
</section>
<section id="how-to-improve-pyspark-s-dataframes-display">
<h2>How To Improve PySpark’s DataFrames Display<a class="headerlink" href="#how-to-improve-pyspark-s-dataframes-display" title="Link to this heading"></a></h2>
<p>When you start working with PySpark DataFrames, you might try “running” the name of the DataFrame object in a cell as you would for a Pandas DataFrame. If you did that, the result probably surprised you.</p>
<p><img alt="The output of a PySpark DataFrame looks like a simple repr method." src="../_images/image-162.png" /></p>
<p>That simple <code class="docutils literal notranslate"><span class="pre">__repr__</span></code> style output was probably not what you wanted to see.</p>
<p>Also, as we’ve already mentioned, the ASCII-style output of the <code class="docutils literal notranslate"><span class="pre">show</span></code> method on the PySpark DataFrame is not very satisfactory if we’re working in a Notebook.</p>
<p>There are a couple of options when working in a notebook to get things looking a little more polished. Probably the best option for Notebook work is to use the following configuration setting:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># For Jupyter Notebook / Lab use only.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.repl.eagerEval.enabled&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This changes how the __repr__ method behaves, so just using the name of the DataFrame gives you a result that looks more like what we see in Pandas. The first twenty rows are shown, but here we show the top five.</p>
<p><img alt="Improving the display of PySpark DataFrames by setting the configuration value spark.sql.repl.eagerEval.enabled." src="../_images/image-17-1024x166.png" /></p>
<p>The second option for improving the display of the PySpark DataFrame also has other uses as well. Here we convert the PySpark DataFrame to one that behaves <em>somewhat</em> like a Pandas DataFrame. Doing this relies on having pandas installed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># One time only:</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">pandas</span>

<span class="n">taxis_pandas</span> <span class="o">=</span> <span class="n">taxis</span><span class="o">.</span><span class="n">pandas_api</span><span class="p">()</span>
<span class="n">taxis_pandas</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="Displaying Spark data using the pandas_api() function." src="../_images/image-18-1024x140.png" /></p>
<p>Note that here we saved the result and called the <code class="docutils literal notranslate"><span class="pre">head</span></code> method on it rather than just using the name (<code class="docutils literal notranslate"><span class="pre">taxis_pandas</span></code>) directly. The pandas_api() function doesn’t emulate Pandas perfectly, so using the raw name shows the first 1,000 rows!</p>
</section>
<section id="writing-parquet-files">
<h2>Writing Parquet Files<a class="headerlink" href="#writing-parquet-files" title="Link to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">write</span></code> attribute of PySpark’s DataFrame object is not a method but a reference to a DataFrameWriter object, which allows you to save the DataFrame to many different formats, including Parquet, CSV, and external or Parquet database formats. The <code class="docutils literal notranslate"><span class="pre">parquet</span></code> method is the one that interests us, and the code is an easy one-liner.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">top_fares</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;./top_fares.parquet&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>If we list the contents of our directory now, we notice that top_fares is not written as a single file but as a folder. If we wait a moment, we can also see this in the sidebar in Jupyter lab.</p>
<p><img alt="JupyterLab SideBar showing Parquet data written as a folder." src="../_images/image-202.png" /></p>
<p>PySpark more or less assumes that you’re going to partition your data set, and if it doesn’t need to do so, it still creates a folder with a single partition, as shown below:</p>
<p><img alt="Structure of a default Pandas file saved by PySpark DataFrame.write.parquet" src="../_images/image-216.png" /></p>
<p>If you really need or want to store this as a single parquet file, you can do so by first converting the DataFrame to Pandas (using <code class="docutils literal notranslate"><span class="pre">toPandas(),</span></code> not <code class="docutils literal notranslate"><span class="pre">pandas_api()</span></code>), then calling the appropriate Pandas method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">top_fares</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span><span class="o">.</span><span class="n">to_parquet</span><span class="p">(</span><span class="s1">&#39;./top_fares_pandas.parquet&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Again, this relies on also having Pandas installed. Unlike pandas_api, the toPandas method returns a true <code class="docutils literal notranslate"><span class="pre">pandas.core.frame.DataFrame</span></code> object, not an imperfect implementation of the Pandas API.</p>
</section>
<section id="partitioning-parquet-files">
<h2>Partitioning Parquet Files<a class="headerlink" href="#partitioning-parquet-files" title="Link to this heading"></a></h2>
<p>PySpark supports partitioning Parquet files to improve the performance of certain queries. For example, suppose you need to run one or more queries summarizing data by where the driver picked up a passenger. In this case, you could partition the data by pickup_borough, allowing the query to be distributed. This can result in a performance boost even on a single machine.</p>
<p>Here’s the code to partition and save the file</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">taxis</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;taxis_by_pickup_borough.parquet&quot;</span><span class="p">,</span> <span class="n">partitionBy</span><span class="o">=</span><span class="s2">&quot;pickup_borough&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>On disk, the structure of the partitioned data looks like this:</p>
<p><img alt="Structure of a partitioned Parquet data file." src="../_images/image-222.png" /></p>
<p>Let’s see how the performance of the query is affected by this change by comparing it to the same query run against the original <code class="docutils literal notranslate"><span class="pre">taxis.parquet</span></code> file. First, let’s look at the unpartitioned case:</p>
<p><img alt="Performance of a PySpark.sql query without partitioning." src="../_images/image-283.png" /></p>
<p>Next, we’ll run the same query against our partitioned data:</p>
<p><img alt="Performance of a PySpark.sql query with partitioning." src="../_images/image-293.png" /></p>
<p>With the partitioned data set, the query took 6.91 microseconds instead of 12.2 microseconds. This sort of savings is noticeable now but even more important as data grows. We have less than 7,000 rows of data in this dataset. It’s enough to prove the case, but it’s hardly “big data.”</p>
<p>Incidentally, the ORC data format also supports partitioning data, and the syntax for writing and querying it parallels that of Parquet, i.e.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">taxis</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">orc</span><span class="p">(</span><span class="s2">&quot;taxis_by_pickup_borough.orc&quot;</span><span class="p">,</span> <span class="n">partitionBy</span><span class="o">=</span><span class="s2">&quot;pickup_borough&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT pickup_borough, count(*) from orc.`taxis_by_pickup_borough.orc` group by pickup_borough&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>At the scales we’re dealing with here, ORC and Parquet offer comparable performance on SELECT queries, but Parquet’s write performance is consistently faster.</p>
</section>
<section id="parquet-vs-pyspark-tables">
<h2>Parquet vs. PySpark Tables<a class="headerlink" href="#parquet-vs-pyspark-tables" title="Link to this heading"></a></h2>
<p>Another argument to consider in favor of Parquet as a data format for PySpark is that it is used by default when you create a Spark table. To see this in action, let’s re-save our taxis DataFrame to a table, partitioning it the same way by pickup_borough.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">taxis</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s2">&quot;taxis_by_pickup&quot;</span><span class="p">,</span> <span class="n">partitionBy</span><span class="o">=</span><span class="s2">&quot;pickup_borough&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>That done, we can now query it by table name, with exactly the simple naming format might expect when using a RDBMS:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT pickup_borough, count(*) as total_fares from taxis_by_pickup group by pickup_borough&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Spark’s tables are stored in the spark-warehouse directory, created automatically when you run PySpark. So we can see our table structure on disk and verify its format as shown below:</p>
<p><img alt="By default, Spark tables are simply parquet files saved in a special directory." src="../_images/image-301.png" /></p>
<p>This shows us that the default data structure used by createTable was Parquet. Moreover, the layout and query performance is quite similar to what we showed before for a partitioned Parquet dataset. The main advantages to saving parquet files as tables are to simplify select queries slightly and to enable easy use of other SQL operations, such as “SHOW TABLES” or “DESCRIBE TABLE taxis_by_pickup.”</p>
</section>
<section id="closing-thoughts">
<h2>Closing Thoughts<a class="headerlink" href="#closing-thoughts" title="Link to this heading"></a></h2>
<p>In the course of our examination of PySpark and Parquet, we saw how easy it is to load and save data into a DataFrame. The interoperability with Pandas means you can be productive right away, while the ability to directly query files in a distributed environment or locally using Spark SQL should be a welcome addition to your toolkit.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../dunder-methods-in-python-the-ugliest-awesome-sauce/" class="btn btn-neutral float-left" title="Python Dunder Methods: The Ugliest Awesome Sauce" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../python-dataclasses/" class="btn btn-neutral float-right" title="Python Dataclass: Easily Automate Class Best Practices" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, John Lockwood.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-QX7KGT4YPE"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-QX7KGT4YPE', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>